version: "2"
data-prepper:
  # Health check and metrics configuration
  server:
    port: 4900
    health_check_service: true
  metrics:
    prometheus:
      port: 9600
  
  # Global error handling configuration
  error_handling:
    # Log level for Data Prepper internal logs
    log_level: "INFO"
    # Error log format (JSON for structured logging)
    log_format: "JSON"
    # Maximum number of processing errors before circuit breaker opens
    max_errors_threshold: 100
    # Circuit breaker reset timeout
    circuit_breaker_timeout: "30s"

# Log processing pipeline
log-pipeline:
  source:
    otel_logs_source:
      port: 21892
      health_check_service: true
      proto_reflection_service: true
      ssl: false
      
  processor:
    # Date processor for timestamp parsing with error handling
    - date:
        match:
          - timestamp: "yyyy-MM-dd'T'HH:mm:ss.SSSXXX"
          - "@timestamp": "yyyy-MM-dd'T'HH:mm:ss.SSSXXX"
        target_key: "@timestamp"
        # Error handling for date parsing failures
        on_parse_failure: "keep_original"
        
    # Mutate processor for field renaming and transformation
    - mutate:
        rename_keys:
          "attributes.service.name": "service_name"
          "attributes.service.version": "service_version"
          "severityText": "severity_text"
          "body": "message"
        add_entries:
          - key: "pipeline_processed"
            value: true
          - key: "processed_at"
            value_expr: 'now()'
        # Error handling for mutate processor
        tags_on_failure: ["_mutate_failure"]
            
    # Grok processor for log parsing with enhanced error handling
    - grok:
        match:
          message: 
            - "%{COMMONAPACHELOG}"
            - "%{COMBINEDAPACHELOG}"
            - "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} %{GREEDYDATA:log_message}"
        target_key: "parsed_log"
        break_on_match: true
        keep_empty_captures: false
        # Error handling for grok parsing failures
        tags_on_failure: ["_grok_parse_failure"]
        timeout_millis: 30000
        
  sink:
    - opensearch:
        hosts: 
          - "http://opensearch:9200"
        index: "logs-%{yyyy.MM.dd}"
        index_type: "_doc"
        template_file: "/usr/share/data-prepper/templates/opensearch-logs-template.json"
        
        # Bulk configuration
        bulk_size: 100
        flush_timeout: 5s
        
        # Authentication (if needed)
        # username: "admin"
        # password: "admin"
        
        # SSL configuration (if needed)
        # ssl: true
        # ssl_certificate_verification: false
        
        # Enhanced retry configuration with exponential backoff
        max_retries: 5
        retry_backoff: "exponential"
        initial_retry_delay: "1s"
        max_retry_delay: "30s"
        retry_multiplier: 2.0
        
        # Connection timeout and socket timeout
        connection_timeout: "10s"
        socket_timeout: "30s"
        
        # Document ID template (optional)
        document_id_field: "trace_id"
        
        # Enhanced Dead Letter Queue configuration
        dlq:
          # DLQ file path with rotation
          file: "/usr/share/data-prepper/data/dlq/failed-logs-%{yyyy-MM-dd-HH}.json"
          # Maximum file size before rotation (100MB)
          max_file_size: "100mb"
          # Maximum number of DLQ files to keep
          max_files: 10
          # DLQ retention policy (7 days)
          retention_days: 7
          # Include original error information
          include_error_details: true
          # DLQ format (JSON Lines for easy processing)
          format: "json_lines"
        
        # Error handling policies
        error_handling:
          # Action on indexing failure: retry -> dlq -> drop
          on_index_failure: "dlq"
          # Action on connection failure: retry -> dlq
          on_connection_failure: "retry"
          # Action on authentication failure: dlq (don't retry auth failures)
          on_auth_failure: "dlq"
          # Action on mapping conflicts: dlq
          on_mapping_conflict: "dlq"
          # Maximum document size (10MB)
          max_document_size: "10mb"
          # Action when document exceeds size limit
          on_oversized_document: "dlq"

# Dead Letter Queue processing pipeline
dlq-reprocessing-pipeline:
  source:
    file:
      path: "/usr/share/data-prepper/data/dlq/failed-logs-*.json"
      format: "json"
      # Process DLQ files every 5 minutes
      scan_interval: "5m"
      # Delete processed DLQ files
      delete_processed_files: false
      # Move processed files to archive
      archive_directory: "/usr/share/data-prepper/data/dlq/archive"
      
  processor:
    # Add DLQ reprocessing metadata
    - mutate:
        add_entries:
          - key: "dlq_reprocessed"
            value: true
          - key: "dlq_reprocessed_at"
            value_expr: 'now()'
          - key: "dlq_attempt_count"
            value_expr: 'getMetadata("attempt_count") + 1'
            
    # Conditional processor to limit reprocessing attempts
    - drop_events:
        drop_when: 'getMetadata("attempt_count") > 3'
        
  sink:
    - opensearch:
        hosts: 
          - "http://opensearch:9200"
        index: "logs-dlq-reprocessed-%{yyyy.MM.dd}"
        index_type: "_doc"
        
        # Reduced retry for DLQ reprocessing
        max_retries: 2
        retry_backoff: "linear"
        initial_retry_delay: "5s"
        
        # Final DLQ for permanently failed logs
        dlq:
          file: "/usr/share/data-prepper/data/dlq/permanent-failures-%{yyyy-MM-dd}.json"
          max_file_size: "50mb"
          max_files: 5
          retention_days: 30
          include_error_details: true
          format: "json_lines"

# Error monitoring and alerting pipeline
error-monitoring-pipeline:
  source:
    # Monitor Data Prepper internal metrics
    internal_metrics:
      interval: "30s"
      
  processor:
    # Filter for error metrics
    - drop_events:
        drop_when: 'getMetadata("metric_type") != "error"'
        
    # Add alerting metadata
    - mutate:
        add_entries:
          - key: "alert_severity"
            value_expr: 'getMetadata("error_count") > 10 ? "high" : "medium"'
          - key: "alert_timestamp"
            value_expr: 'now()'
            
  sink:
    # Send error metrics to OpenSearch for monitoring
    - opensearch:
        hosts: 
          - "http://opensearch:9200"
        index: "data-prepper-errors-%{yyyy.MM.dd}"
        index_type: "_doc"
        
        # Minimal retry for error monitoring (avoid cascading failures)
        max_retries: 1
        retry_backoff: "linear"
        
    # Also send to stdout for immediate visibility
    - stdout:
        format: "json"

# Additional pipeline for metrics (if needed)
metrics-pipeline:
  source:
    otel_metrics_source:
      port: 21891
      health_check_service: true
      
  processor:
    # Add error handling for metrics processing
    - mutate:
        add_entries:
          - key: "pipeline_type"
            value: "metrics"
          - key: "processed_at"
            value_expr: 'now()'
        tags_on_failure: ["_metrics_mutate_failure"]
      
  sink:
    - opensearch:
        hosts: 
          - "http://opensearch:9200"
        index: "metrics-%{yyyy.MM.dd}"
        index_type: "_doc"
        
        # Error handling for metrics
        max_retries: 3
        retry_backoff: "exponential"
        
        dlq:
          file: "/usr/share/data-prepper/data/dlq/failed-metrics-%{yyyy-MM-dd}.json"
          max_file_size: "50mb"
          max_files: 5
          retention_days: 7
          include_error_details: true
          format: "json_lines"